# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lqnEndx6qeUBT5lIhh42S3toBgk4FG1k
"""

import os

# Inspect the directory structure
for root, dirs, files in os.walk(path):
    # Only print directories that actually contain files to keep it clean
    if files:
        print(f"Folder: {root}")
        print(f"Sample files: {files[:3]}") # Show first 3 files
        print("-" * 30)

import os
import random
import shutil
import json

# 1. Path setup based on your discovery
path = "/root/.cache/kagglehub/datasets/hendrichscullen/vehide-dataset-automatic-vehicle-damage-detection/versions/1"
output_root = "./vehicle_damage_subset"

# Source paths from your Discovery output
src_train_images = os.path.join(path, 'image', 'image')
src_val_images = os.path.join(path, 'validation', 'validation')
train_json = os.path.join(path, '0Train_via_annos.json')
val_json = os.path.join(path, '0Val_via_annos.json')

# ------------------------------------------
# Step 1: Dataset Overview
# ------------------------------------------
all_train_images = [f for f in os.listdir(src_train_images) if f.endswith(('.jpg', '.jpeg', '.png'))]
all_val_images = [f for f in os.listdir(src_val_images) if f.endswith(('.jpg', '.jpeg', '.png'))]

print(f"Total Train images: {len(all_train_images)}")
print(f"Total Val images: {len(all_val_images)}")

# ------------------------------------------
# Step 2 & 3: Subset Selection
# ------------------------------------------
# Since the dataset is already split, we'll take a subset of both
TRAIN_SUBSET_SIZE = 160
VAL_SUBSET_SIZE = 40

random.seed(42)
selected_train = random.sample(all_train_images, min(TRAIN_SUBSET_SIZE, len(all_train_images)))
selected_val = random.sample(all_val_images, min(VAL_SUBSET_SIZE, len(all_val_images)))

# ------------------------------------------
# Step 4: Folder Structure Creation
# ------------------------------------------


def create_folders():
    # Creating the structure: dataset/images/train, dataset/labels/train, etc.
    subfolders = ['images/train', 'images/val', 'labels/train', 'labels/val']
    for folder in subfolders:
        os.makedirs(os.path.join(output_root, folder), exist_ok=True)

create_folders()

def move_images(file_list, src_dir, folder_type):
    count = 0
    for filename in file_list:
        src_path = os.path.join(src_dir, filename)
        dest_path = os.path.join(output_root, 'images', folder_type, filename)
        shutil.copy2(src_path, dest_path)
        count += 1
    return count

# Move the subset images
train_count = move_images(selected_train, src_train_images, 'train')
val_count = move_images(selected_val, src_val_images, 'val')

# Copy the JSON labels into the labels folder
shutil.copy2(train_json, os.path.join(output_root, 'labels', 'train', 'annotations.json'))
shutil.copy2(val_json, os.path.join(output_root, 'labels', 'val', 'annotations.json'))

print(f"\n--- Subset Preparation Complete ---")
print(f"Images moved: {train_count} train, {val_count} val")
print(f"Labels: JSON annotation files copied to labels/train and labels/val")

import json
import os

# Function to keep only the 200 selected images in the JSON metadata
def filter_json_annotations(json_path, selected_filenames):
    with open(json_path, 'r') as f:
        data = json.load(f)

    filtered_data = {}

    # We use 'name' because your "Peek" step confirmed that is the key used
    for key, value in data.items():
        if isinstance(value, dict) and 'name' in value:
            if value['name'] in selected_filenames:
                filtered_data[key] = value

    with open(json_path, 'w') as f:
        json.dump(filtered_data, f)

    return len(filtered_data)

# Define the paths to the JSON files inside your NEW subset folder
train_json_subset = os.path.join(output_root, 'labels/train/annotations.json')
val_json_subset = os.path.join(output_root, 'labels/val/annotations.json')

# Run the filtering
final_train_count = filter_json_annotations(train_json_subset, selected_train)
final_val_count = filter_json_annotations(val_json_subset, selected_val)

print(f"--- Data Cleanliness Check ---")
print(f"Train annotations successfully filtered to: {final_train_count}")
print(f"Val annotations successfully filtered to: {final_val_count}")

import json

# Let's peek at the first entry of the training JSON
json_path = os.path.join(output_root, 'labels/train/annotations.json')
with open(json_path, 'r') as f:
    data = json.load(f)

# Get the first key and its contents
first_key = list(data.keys())[0]
print(f"First Key in JSON: {first_key}")
print(f"Content of that key: {data[first_key]}")

import matplotlib.pyplot as plt
import cv2
import json

# Load one image and its labels from your NEW subset
sample_img_name = selected_train[0]
img_path = os.path.join(output_root, 'images/train', sample_img_name)
json_path = os.path.join(output_root, 'labels/train/annotations.json')

with open(json_path, 'r') as f:
    annotations = json.load(f)

# Find the annotation for this specific image
# Note: In your JSON, the key might be the filename itself
anno = annotations.get(sample_img_name)

img = cv2.imread(img_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

plt.figure(figsize=(10, 10))
plt.imshow(img)

# Plot the polygon points
for region in anno['regions']:
    x = region['all_x']
    y = region['all_y']
    plt.plot(x, y, 'r-', linewidth=2) # Draw red line
    plt.scatter(x, y, s=10, color='yellow') # Draw yellow dots at corners

plt.title(f"Verifying Labels: {sample_img_name}")
plt.axis('off')
plt.show()
